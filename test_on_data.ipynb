{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic import data to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading\n",
      "done separate x and y\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# import data\n",
    "print('start reading')\n",
    "f = open(\"TraData.csv\")\n",
    "data = np.loadtxt(f, delimiter=',')\n",
    "\n",
    "# select data\n",
    "x = data[:, 1:]  # select columns 1 through end\n",
    "y = data[:, 0]   # select column 0, the result\n",
    "print('done separate x and y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n",
    "- first normalize data and scaling \n",
    "- then select 95 % important thing out of the data \n",
    "    - select percentile\n",
    "- Use PCA to take data\n",
    "- disjoint training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 31)\n",
      "(500000, 29)\n",
      "(500000, 29)\n",
      "start2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "normalizer = preprocessing.Normalizer().fit(x)\n",
    "X_scaled = preprocessing.RobustScaler().fit(x)\n",
    "normalizer.transform(x)\n",
    "X_scaled.transform(x)\n",
    "print(x.shape)\n",
    "\n",
    "#for i in range(31):\n",
    "#    print(x[0][i])\n",
    "X_new = SelectPercentile(percentile=95).fit_transform(x, y)\n",
    "print(X_new.shape)\n",
    "#for i in range(29):\n",
    "#    print(X_new[0][i])\n",
    "\n",
    "pca = PCA(n_components=29)\n",
    "X_pca = pca.fit_transform(X_new)\n",
    "print(X_pca.shape)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "\n",
    "#train_x, valid_x,train_y,valid_y = train_test_split(X_pca,y,test_size=0.1)\n",
    "#train_x_2, valid_x_2,train_y_2,valid_y_2 = train_test_split(x,y,test_size=0.1)\n",
    "print('start2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start random forest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.70903658, 0.71174258, 0.70818883])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print('start random forest')\n",
    "# random forest\n",
    "#clf2 = RandomForestClassifier(criterion='entropy',\n",
    "#                                  n_estimators=50)\n",
    "clf2 = RandomForestClassifier(criterion='entropy',max_depth = 20,\n",
    "                                  n_estimators=200)\n",
    "#clf2 = clf2.fit(train_x_2,train_y_2)\n",
    "\n",
    "#print('Training Acc:\\n',clf2.score(train_x_2,train_y_2))\n",
    "#print('Training data predict:\\n',clf2.predict(train_x_2))\n",
    "#print('Validate Accuracy:\\n',clf2.score(valid_x_2,valid_y_2))\n",
    "#print('Validate data predict:\\n',clf2.predict(valid_x_2))\n",
    "\n",
    "\n",
    "clf2 = clf2.fit(X_pca, y)\n",
    "cv_results = cross_validate(clf2, X_pca, y, cv=3)\n",
    "sorted(cv_results.keys())\n",
    "# ['fit_time', 'score_time', 'test_score']\n",
    "cv_results['test_score']\n",
    "\n",
    "# Save Model:\n",
    "#f = open('clf2_Random_Forest_1.pickle', 'wb')\n",
    "#pickle.dump(clf2, f)\n",
    "#f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuro network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start neuro network\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.74294851, 0.74620051, 0.74166297])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf4 = MLPClassifier(activation='relu',solver='sgd',max_iter=2000,learning_rate_init=0.001,\n",
    "                    hidden_layer_sizes=(45,35,35,25)) \n",
    "print('start neuro network')\n",
    "#clf4 = clf4.fit(train_x,train_y)\n",
    "\n",
    "clf4 = clf4.fit(X_pca, y)\n",
    "cv_results = cross_validate(clf4, X_pca, y, cv=3)\n",
    "sorted(cv_results.keys())\n",
    "# ['fit_time', 'score_time', 'test_score']\n",
    "cv_results['test_score']\n",
    "\n",
    "#print('Training Acc:\\n',clf4.score(train_x,train_y))\n",
    "#print('Training data predict:\\n',clf4.predict(train_x))\n",
    "#print('Validate Accuracy:\\n',clf4.score(valid_x,valid_y))\n",
    "#print('Validate data predict:\\n',clf4.predict(valid_x))\n",
    "\n",
    "# Save Model:\n",
    "#f = open('clf4_NN_CY_2.pickle', 'wb')\n",
    "#pickle.dump(clf4, f)\n",
    "#f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print('start K_Neighbour_Classifier')\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh = neigh.fit(train_x,train_y)\n",
    "\n",
    "print('Training Acc:\\n',neigh.score(train_x,train_y))\n",
    "print('Training data predict:\\n',neigh.predict(train_x))\n",
    "print('Validate Accuracy:\\n',neigh.score(valid_x,valid_y))\n",
    "print('Validate data predict:\\n',neigh.predict(valid_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start voting soft\n",
      "Training Acc:\n",
      " 0.9753933333333333\n",
      "Training data predict:\n",
      " [0. 1. 0. ... 1. 1. 1.]\n",
      "Validate Accuracy:\n",
      " 0.73232\n",
      "Validate data predict:\n",
      " [0. 1. 0. ... 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "print('start voting soft')\n",
    "eclf2 = VotingClassifier(\n",
    "       estimators=[('NN', clf4), ('r_f', clf2),('ada_boost',clf)],\n",
    "       voting='soft')\n",
    "eclf2.fit(train_x,train_y)\n",
    "\n",
    "print('Training Acc:\\n',eclf2.score(train_x,train_y))\n",
    "print('Training data predict:\\n',eclf2.predict(train_x))\n",
    "print('Validate Accuracy:\\n',eclf2.score(valid_x,valid_y))\n",
    "print('Validate data predict:\\n',eclf2.predict(valid_x))\n",
    "\n",
    "\n",
    "# Save Model:\n",
    "f = open('cy_Vote_sot1.pickle', 'wb')\n",
    "pickle.dump(eclf2, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "MLPClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-00eb9843bbd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_pca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# Clear any previous fit results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sample_weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[1;32m--> 456\u001b[1;33m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: MLPClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf4 = MLPClassifier(activation='relu',solver='sgd',max_iter=2000,learning_rate_init=0.001,\n",
    "                    hidden_layer_sizes=(45,35))\n",
    "clf = AdaBoostClassifier(clf4, n_estimators=20)\n",
    "\n",
    "clf = clf.fit(X_pca, y)\n",
    "cv_results = cross_validate(clf, X_pca, y, cv=3)\n",
    "sorted(cv_results.keys())\n",
    "# ['fit_time', 'score_time', 'test_score']\n",
    "cv_results['test_score']\n",
    "\n",
    "\n",
    "#clf=clf.fit(train_x,train_y)\n",
    "#print('Training Acc:\\n',clf.score(train_x,train_y))\n",
    "#print('Training data predict:\\n',clf.predict(train_x))\n",
    "#print('Validate Accuracy:\\n',clf.score(valid_x,valid_y))\n",
    "#print('Validate data predict:\\n',clf.predict(valid_x))\n",
    "\n",
    "\n",
    "# Save Model:\n",
    "#f = open('cy_ada_boost.pickle', 'wb')\n",
    "#pickle.dump(clf, f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n",
      "output file saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "infile = pd.read_csv('input.csv', header=None).values\n",
    "infile = np.array(infile)\n",
    "\n",
    "x = infile\n",
    "normalizer = preprocessing.Normalizer().fit(x)\n",
    "X_scaled = preprocessing.RobustScaler().fit(x)\n",
    "normalizer.transform(x)\n",
    "X_scaled.transform(x)\n",
    "print(x.shape)\n",
    "\n",
    "#X_new = np.c_[x[:,0], x[:,2:20]]\n",
    "#X_new = np.c_[X_new, x[:,21:]]\n",
    "#print(X_new.shape)\n",
    "\n",
    "#X_new = np.c_[x[:,0], x[:,2:20]]\n",
    "#X_new = np.c_[X_new, x[:,21:]]\n",
    "#print(X_new.shape)\n",
    "\n",
    "#pca = PCA(n_components=29)\n",
    "#X_pca = pca.fit_transform(x)\n",
    "#print(X_pca.shape)\n",
    "\n",
    "f = open('vote_hard_ensemble.pickle', 'rb')\n",
    "trained_cf = pickle.load(f)\n",
    "f.close()\n",
    "#print(trained_cf.score(valid_x,valid_y))\n",
    "np.savetxt('output.csv', trained_cf.predict(x), fmt='%d')\n",
    "print('output file saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 29)\n",
      "(500000, 29)\n",
      "0.951086\n",
      "(500000, 29)\n",
      "0.497502\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "f = open('vote_hard_ensemble.pickle', 'rb')\n",
    "trained_cf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#X_new = np.c_[x[:,0], x[:,2:20]]\n",
    "#X_new = np.c_[X_new, x[:,21:]]\n",
    "#print(X_new.shape)\n",
    "#pca = PCA(n_components=29)\n",
    "#X_pca = pca.fit_transform(X_new)\n",
    "#print(X_pca.shape)\n",
    "\n",
    "print(trained_cf.score(X_pca,y))\n",
    "\n",
    "#pca = PCA(n_components=29)\n",
    "#X_pca = pca.fit_transform(x)\n",
    "#print(X_pca.shape)\n",
    "\n",
    "#print(trained_cf.score(X_pca,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final combination ！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading\n",
      "done separate x and y\n",
      "(500000, 31)\n",
      "start random forest\n",
      "start neuro network\n",
      "start voting soft\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# import data\n",
    "print('start reading')\n",
    "f = open(\"TraData.csv\")\n",
    "data = np.loadtxt(f, delimiter=',')\n",
    "\n",
    "# select data\n",
    "x = data[:, 1:]  # select columns 1 through end\n",
    "y = data[:, 0]   # select column 0, the result\n",
    "print('done separate x and y')\n",
    "\n",
    "normalizer = preprocessing.Normalizer().fit(x)\n",
    "X_scaled = preprocessing.RobustScaler().fit(x)\n",
    "normalizer.transform(x)\n",
    "X_scaled.transform(x)\n",
    "print(x.shape)\n",
    "\n",
    "print('start random forest')\n",
    "clf1 = RandomForestClassifier(criterion='entropy',max_depth = 20,\n",
    "                                  n_estimators=200)\n",
    "print('start neuro network')\n",
    "clf2 = MLPClassifier(activation='relu',solver='sgd',max_iter=2000,learning_rate_init=0.001,\n",
    "                    hidden_layer_sizes=(45,35,35)) \n",
    "\n",
    "clf3 = MLPClassifier(activation='relu',solver='adam', #adam, 39, 39, 74.4\n",
    "                     max_iter=1000,\n",
    "                     hidden_layer_sizes=(39, 39, 39),#best: 50, 50?\n",
    "                     random_state=None)\n",
    "\n",
    "print('start voting soft')\n",
    "fclf = VotingClassifier(\n",
    "       estimators=[('RF', clf1), ('NN1', clf2),('NN2',clf3)],\n",
    "       voting='hard')\n",
    "fclf.fit(x,y)\n",
    "print('end voting soft')\n",
    "\n",
    "# Save Model:\n",
    "f = open('vote_hard_ensemble.pickle', 'wb')\n",
    "pickle.dump(fclf, f)\n",
    "f.close()\n",
    "print('end saving data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
